{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a331300-5ff3-4219-83c0-ce11703cc9e1",
   "metadata": {},
   "source": [
    "# How To Use Vertex Gemini for Multimodal Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34429b82-078c-4107-827d-8aaed4ab1121",
   "metadata": {},
   "source": [
    "This notebook outlines how to interact with Vertex AI's Gemini Vision Pro GenAI model to inspect images and generate detailed information about its content. Visual Question Answering (VQA) lets you provide an image to the model and ask a question about the image's contents. In response to your question you get one or more natural language answers. In this example we will identify products in an image, output the description and item details in a CSV format and then create embeddings using the Gemini embeddings API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ed54ec-9f36-40af-8d2e-c6fd32a76222",
   "metadata": {},
   "source": [
    "## Prepare the python development environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906ce888-8c52-476b-96d0-7f65cb259f8d",
   "metadata": {},
   "source": [
    "First, let's identify any project specific variables to customize this notebook to your GCP environment. Change YOUR_PROJECT_ID with your own GCP project ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205088b5-6d5b-46dd-9d77-a8352eac630b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#PROJECT_ID = 'YOUR_PROJECT_ID'\n",
    "PROJECT_ID = 'rkiles-demo-host-vpc'\n",
    "LOCATION = 'us-central1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e7d4bb-dd35-4477-b94a-8666cf0f9695",
   "metadata": {},
   "source": [
    "Next, let's specify the name of the image file you want to inspect, such as \"stuff_on_a_shelf.jpg\" or \"shoe.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba226a4-d673-4778-8cbd-c32dc88b1389",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filename = 'stuff_on_a_shelf.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe92738a-045b-48a4-ab0e-1d98fad2089a",
   "metadata": {},
   "source": [
    "Install any needed python modules from our requirements.txt file. Most Vertex Workbench environments include all the packages we'll be using, but if you are using an external Jupyter Notebook or require any additional packages for your own needs, you can simply add them to the included requirements.txt file an run the folloiwng commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69175b8-63c4-4b5a-837d-055743c0b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444b2ce7-0277-4ada-9648-34c67774bfd2",
   "metadata": {},
   "source": [
    "Now we will import all required modules. For our purpose, we will be utilizing the following:\n",
    "\n",
    "- google.auth - Provides authentication access to the Google API's, such as imagegeneration:predict\n",
    "- base64 - Imagen API requests return generated or edited images as base64-encoded strings. This module will help us decode this data to an image file\n",
    "- requests - This module will allow us to interact directly with Imagen over the REST API. \n",
    "- json - Python module used to interact with JSON data. Imagen returns results in json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e15897-0cb0-4173-9b91-17270a10b989",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.auth.transport.requests\n",
    "import google.auth\n",
    "import base64\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2b2668-962a-42da-b92b-4932f224bb0d",
   "metadata": {},
   "source": [
    "## Authenticate to the Vertex AI API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef57a2c4-d51a-42b8-b0c7-24aced49d64b",
   "metadata": {},
   "source": [
    "Our Vertex Workbench instance is configured to use a specified service account that has IAM access to the Gemini Vison Pro API. The following two secitons will allow us to generate the access token we will pass as an authorization bearer token later in the header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997bec4d-c8d6-4f90-9726-3e574ab061d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials, project_id = google.auth.default()\n",
    "auth_req = google.auth.transport.requests.Request()\n",
    "credentials.refresh(auth_req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5d12eb-fd5c-4c70-b278-6a199985f3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = credentials.token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3fb974-e836-40c5-81e3-c83f63bc84e0",
   "metadata": {},
   "source": [
    "## Prepare the HTTP POST request to the REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93462b3-7ee7-4f6f-a141-0d54ce304669",
   "metadata": {},
   "source": [
    "Define the header fields, including the access token we created in the last step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32a6c60-9040-4f58-9d25-383e25b86360",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "        'Authorization': 'Bearer ' + access_token,\n",
    "        'Content-Type': 'application/json; charset=utf-8'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00a60a7-400c-4c54-989b-6e5a9b931711",
   "metadata": {},
   "source": [
    "You can uncomment the following line for troubleshooting if you want to see how the header will be passed to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4842b3-abb5-4941-b501-a78bd6341306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5192001a-47ec-4a6b-a93e-f91ce2e1dd79",
   "metadata": {},
   "source": [
    "Next we will specifiy the URL for the Imagen REST API. You should have already specified the correct project ID in the very first step of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfea6f4-3cbd-413c-9996-3edd9ee944e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f'https://{LOCATION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/publishers/google/models/gemini-pro-vision:streamGenerateContent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19708fb0-95cd-4f69-bd7c-0c68340471d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_url = f'https://{LOCATION}-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/{LOCATION}/publishers/google/models/multimodalembedding:predict'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312d82f2-ab09-4da5-ac82-6b659c765051",
   "metadata": {},
   "source": [
    "To use Gemini Vision Pro on Vertex AI you must provide a text description of what you want to inspect, generate or edit. These descriptions are called prompts, and these prompts are the primary way you communicate with Generative AI. Here, we are specifiying what we want the model to identify using a prompt. Play around with this content and see what kind of details you can extract from an image. More information can be found here https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a72533f-a173-4082-8ab7-db61a6afc47f",
   "metadata": {},
   "source": [
    "In this example, we will ask Gemini to inspect a picture of an orange juice carton and provide it's results in a json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f16782-6804-4ea6-b44b-00ea66c05710",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#vqa_prompt = 'Briefly describe each product you see in this picture and provide your response in JSON format including the brand, description, price and size. If you can not determine the size, mark it as NA. Do not include the json prefix in your response.'\n",
    "\n",
    "vqa_prompt = 'Briefly describe each product you see in this picture. Include the brand, description, price, size and item number. If you can not determine the size, mark it as NA. Format the output as a csv with each item on a different row'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c7cade-d8da-4372-b9d9-608ae0f9a8a7",
   "metadata": {},
   "source": [
    "Next we will specify the mime type and locaiton of the image file we want to inspect. The example below uses a local file named stuff_on_a_shelf.jpg. More information can be found at https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315e4a16-cee8-4f01-97f4-e42b6674a787",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(image_filename, \"rb\") as f:\n",
    "    encoded_base_image = base64.b64encode(f.read())\n",
    "B64_BASE_IMAGE = encoded_base_image.decode('utf-8')\n",
    "\n",
    "image_file = '\"data\": \"'+ B64_BASE_IMAGE +'\"'\n",
    "mime_type = '\"mimeType\": \"image/png\"'\n",
    "\n",
    "image_file_data = '{\"inlineData\": {' + mime_type +','+ image_file +'}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbc8f6e-52f7-4015-b15c-dc28ba115acb",
   "metadata": {},
   "source": [
    "Instead of using a local image, you can optionally provide the location of an image stored in a GCS bucket as outlined below. Use the format gs://BUCKET_NAME/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378941ff-ad06-490a-a46a-eb2db2a3397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GCS_BUCKET = 'YOUR_BUCKET_PATH'\n",
    "#image_file = '\"fileUri\": \"'+ GCS_BUCKET + image_filename +'\"'\n",
    "#mime_type = '\"mimeType\": \"image/png\"'\n",
    "\n",
    "#image_file_data = '{\"fileData\": {' + mime_type +','+ image_file +'}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1980e70-eef9-49e7-a66e-e4c1bff5cd66",
   "metadata": {},
   "source": [
    "We will now create the request body that will be passed to the REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4152412-a31b-4fa2-aa0a-280f5115c782",
   "metadata": {},
   "outputs": [],
   "source": [
    "request_body = '{\"contents\": {\"role\": \"user\",\"parts\": ['+ image_file_data +',{\"text\": \"' + vqa_prompt + '\"}]},\"generation_config\": {\"maxOutputTokens\": 2048,\"temperature\": 0.4,\"topP\": 1.0,\"topK\": 32}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2227aa1f-cfe3-42d7-8ffd-713934aef13b",
   "metadata": {},
   "source": [
    "Lastly, we will post the request to the Imagen REST API and wait for the requested response to be generated and returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d561c8ec-cfbe-40c8-8fd1-b516d03ed545",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post(url, data=request_body, headers=headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c44c6c-adcd-4418-b805-8be482304e1e",
   "metadata": {},
   "source": [
    "You can optionally uncomment the following to view the returned status code for verification or troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cfcca9-20e4-477d-adfa-3cc66fc3c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.status_code)\n",
    "#print(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cea8b0f-41dc-457e-a23b-81e98abe6c5c",
   "metadata": {},
   "source": [
    "## Process the returned request and decode the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9f2887-4e6a-444f-91d2-355b0ac47422",
   "metadata": {},
   "source": [
    "The Imagen API returns the prediciton in a JSON string. We will start by defining our data and then extracting the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f8359a-fcc6-4952-9f4d-e35c356a4629",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce28a66b-b20c-4ba1-81b6-fb91ae89ddc9",
   "metadata": {},
   "source": [
    "You can optionally uncomment the following to view the returned json payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183e1e10-410b-47d0-994a-1d022d193a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d266e0a3-55e2-4999-92b5-27f161d1b059",
   "metadata": {},
   "source": [
    "As you can see form the full display of the json response by uncommenting the previous line, the Gemini_Vison_Pro model can return the text in multiple sections of the array based on the image and input prompt used. To better process the returned text, we will insert a simple 'for loop' here to iterate through multiple predictions. We will then create embeddings for each of the products. You can optionally view the structure of the CSV data by reviewing the info.csv file that is generated at the beginning of this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed3379a-7673-41ed-8ccc-3e08bfa22416",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('info.csv', 'w') as file:\n",
    "    for candidate in img_data:\n",
    "        file.write(candidate['candidates'][0]['content']['parts'][0]['text'])\n",
    "\n",
    "qa_response = ''\n",
    "for candidate in img_data:\n",
    "    qa_response = qa_response.lstrip() + candidate['candidates'][0]['content']['parts'][0]['text']\n",
    "    req_body = '{\"instances\": [{\"text\": \"'+qa_response+'\"}],\"parameters\": {\"dimension\": 128}}'\n",
    "    r_emb = requests.post(emb_url, data=req_body, headers=headers)\n",
    "    print(r_emb.status_code)\n",
    "    emb_data = r_emb.json()\n",
    "    print(emb_data)\n",
    "    \n",
    "print(qa_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5e2d0b-8755-48c9-8d64-7a0ab8be5ac7",
   "metadata": {},
   "source": [
    "That's it! Congratulations on defining your first visual Q&A with Gemini and creating your embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccafab7-1db3-42c0-83fd-2fd9f3174600",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text_body = f'Here is an example of the product details - {qa_response}'\n",
    "\n",
    "#req_body = '{\"instances\": [{\"text\": \"'+text_body+'\", \"image\": {\"bytesBase64Encoded\": \"'+ B64_BASE_IMAGE +'\"}}],\"parameters\": {\"dimension\": 128}}'\n",
    "\n",
    "#--WORKS--#req_body = '{\"instances\": [{\"image\": {\"bytesBase64Encoded\": \"'+ B64_BASE_IMAGE +'\"}}],\"parameters\": {\"dimension\": 128}}'\n",
    "#print(req_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b02771e-a62f-4276-bc2c-f2e04125b079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#r_emb = requests.post(emb_url, data=req_body, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a39321-1ae7-4531-8168-c7cf5bac28b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(r_emb.status_code)\n",
    "#print(r_emb.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2c0cb0-63cd-4a19-bce9-ff053dfb2121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#emb_data = r_emb.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34867d74-a6c0-4422-a00b-1a12e77462c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(emb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27df4bc8-223e-4901-9b0c-64100f072683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a unique id for this session\n",
    "from datetime import datetime\n",
    "UID = datetime.now().strftime(\"%m%d%H%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ad4a8-a9d8-41b3-845a-849557e55288",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://{PROJECT_ID}-vs-quickstart-{UID}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5ec5b1-685b-441c-b625-0b194e00a409",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m113"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
